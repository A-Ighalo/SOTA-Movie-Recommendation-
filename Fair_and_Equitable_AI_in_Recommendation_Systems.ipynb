{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fair and Equitable AI in Recommendation Systems**\n",
        "## **Kyanon Digital**\n",
        "### **Abraham Ighalo - Data Engineer**\n",
        "#### **April 2025**\n",
        "\n"
      ],
      "metadata": {
        "id": "9dTUwE_cUsDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction\n",
        "**Recommendation systems** play a central role in shaping user experiences across digital platforms, including e-commerce, media streaming, social networks, and hiring platforms. As these systems increasingly influence access to information, opportunities, and resources, it becomes critical to ensure that they operate in a fair and equitable manner.\n",
        "\n",
        "**Fairness in Recommendation Systems**\n",
        "**Fairness** in recommendation systems refers to the absence of unjust or systematic bias in the way content, opportunities, or resources are suggested to users.\n",
        "\n",
        "**Equity in Recommendation Systems**\n",
        "**Equity** in recommendation systems involves proactively addressing structural disadvantages and promoting inclusivity through algorithmic design and policy.\n"
      ],
      "metadata": {
        "id": "1YncNcJRU06M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7rsIL8cbSjN"
      },
      "source": [
        "# MLFlow Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KyjMl0nI3i47",
        "outputId": "216eb44d-b685-47b3-a866-b3b6fb2ec01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "collapsed": true,
        "id": "bLb53YQ7fGfu",
        "outputId": "0c39d891-cbec-4b6e-98d2-dfd8652342b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mlflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-37a2526696db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set Up MLflow Tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Set Up MLflow Tracking\n",
        "import mlflow\n",
        "import subprocess\n",
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\",MLFLOW_TRACKING_URI])\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "mlflow.set_experiment(\"Advanced_Algorithms_Attention___GOD ABEG\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czmiMqeUfVhi"
      },
      "outputs": [],
      "source": [
        "#3.Expose MLflow UI using ngrok\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fIrW_368fc-y"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "# Authenticate ngrok\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "port = 5000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mU3Hs2E_0kq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be6eb7e5-45c5-44e7-bf5f-8939c2c0a5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-88df0503898e>\", line 6, in <cell line: 0>\n",
            "    import surprise\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/surprise/__init__.py\", line 6, in <module>\n",
            "    from .prediction_algorithms import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n",
            "    from .algo_base import AlgoBase\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n",
            "    from .. import similarities as sims\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-88df0503898e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/surprise/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .prediction_algorithms import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/surprise/prediction_algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malgo_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbaseline_only\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mco_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/surprise/prediction_algorithms/algo_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaseline_als\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPredictionImpossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/surprise/similarities.pyx\u001b[0m in \u001b[0;36minit surprise.similarities\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Installing Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import surprise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if surprise doesn't work\n",
        "\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "G7cfkGrEXiEc",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "5c32dd41-9d7a-4c4f-dba9-11dbbd9d8e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m228.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "78155ddffe694d7883e407292fe39341"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a method of analyzing datasets to understand their main characteristics, often using visualization and statistical techniques to summarize data, identify patterns, and reveal relationships. It's an iterative process that helps in gaining initial insights and formulating hypotheses for further analysis. \""
      ],
      "metadata": {
        "id": "No7F7qfAddIt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQ1MFITi8JR"
      },
      "source": [
        "**Step 1**: Understand the Problem and Data\n",
        "**Goal**: Build a recommendation system with filtering criteria that avoids bias from demographic factors, sex, orientation, and popularity\n",
        "\n",
        "**Data Types**:\n",
        "* **Numerical**: discounted_price, actual_price, discount_percentage, rating, rating_count\n",
        "* **Categorical**: product_id, category, user_id, review_id\n",
        "* **Text**: product_name, about_product, user_name, review_title, review_content\n",
        "* **URL**: img_link, product_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GgtPYLDqm8Jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6a0471-af00-4980-f212-445597027a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/amazon-sales-dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karkavelrajaj/amazon-sales-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU6MGLhAjRQc"
      },
      "source": [
        "### **Step 2: Import and Inspect Data**\n",
        "* Load data carefully\n",
        "* Check data size, missing values, data types\n",
        "* Identify errors or inconsistencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOONL_z4oV2X"
      },
      "outputs": [],
      "source": [
        "#save dataset into a dataframe\n",
        "dataset = path +\"/amazon.csv\"\n",
        "df = pd.read_csv(dataset)\n",
        "\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "5EQM63RyN1w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mIj_mP2apkQ0"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "select_top_rows = df.iloc[0:5]\n",
        "print(tabulate(select_top_rows, headers = 'keys', tablefmt = 'grid'))\n",
        "\n",
        "#review datatypes\n",
        "print(f\"\\n\\n\\nAmazon Dataset Datatypes:  \\n\\n{df.dtypes}\")\n",
        "\n",
        "\n",
        "print( f\"\\n\\nHere we have the technical view of the data: \\n\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsaonQk2OmSe"
      },
      "outputs": [],
      "source": [
        "#Descriptive analysis\n",
        "description = df.describe(include = 'object')\n",
        "print(tabulate(description, headers = 'keys', tablefmt = 'grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4mCfCRKgQEl"
      },
      "source": [
        "From the descriptive analysis we know the following :\n",
        "* There are a total of 1465 products in this dataset and by\n",
        "* 1194 unique users( note a user can review multiple items),\n",
        "* we have 211 unique product categories\n",
        "* 550 discounted products\n",
        "The most rated product is the Fire-Boltt Ninja Call Pro Plus 1.83\"\n",
        "\n",
        "\n",
        "https://www.amazon.com/Wayona-Braided-Lightning-Charging-Compatible/dp/B07JW9H4J1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjbD6s8XjbtB"
      },
      "source": [
        "### **Step 3: Handle Missing Data**\n",
        "* Understand missing data patterns (MCAR, MAR, MNAR)\n",
        "* Choose strategy: removal or imputation\n",
        "* Apply suitable imputation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nt4keUbxZqnA"
      },
      "outputs": [],
      "source": [
        "#Check for duplictes and misisng values\n",
        "print(f\"Duplicates: {df.duplicated().sum()} \\n\")\n",
        "print(f\"Missing Values: \\n{df.isnull().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6mP64rdshgL"
      },
      "source": [
        "### **Step 4: Explore Data Characteristics**\n",
        "* Change data types to support calculations\n",
        "* Calculate summary statistics for numerical variables\n",
        "\n",
        "\n",
        "### **Step 5: Perform Data Transformation**\n",
        "\n",
        "* Prepare data for accurate analysis and modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tGQFnUxLHjCq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Removed currency symbols (₹, ,) and converted data types for:\n",
        "- rating_count → integer (removed commas, filled NaNs with 0)\n",
        "- discounted_price → float (removed ₹ and commas)\n",
        "- actual_price → float (removed ₹ and spaces)\n",
        "'''\n",
        "\n",
        "\n",
        "df['rating_count'] = df['rating_count'].str.replace(\",\", \"\").fillna(0).astype(int)\n",
        "df[\"discounted_price\"] = df[\"discounted_price\"].str.replace(r\"[₹,]\",\"\", regex = True).astype(float)\n",
        "df['discount_percentage'] = df['discount_percentage'].str.replace(\"%\",\"\").astype(int)/100\n",
        "df[\"actual_price\"] = df[\"actual_price\"].str.replace(r\"[₹ ,]\",\"\",regex = True).astype(float)\n",
        "\n",
        "\n",
        "# Verify change\n",
        "top_5 = df.iloc[0:5]\n",
        "bottom_5 = df.iloc[-5:]\n",
        "\n",
        "\n",
        "print(tabulate(top_5 , headers = 'keys', tablefmt = 'grid'))\n",
        "print(tabulate(bottom_5, headers = 'keys', tablefmt = 'grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkwVC6nn61dD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "After fixing the data types,\n",
        "check for inflated discounts where discounted_price > actual_price.\n",
        "'''\n",
        "df[df[\"discounted_price\"] > df['actual_price']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7WrO-88fJ1ao"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "summary of the rating column\n",
        "'''\n",
        "df['rating'].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI2lSbFnOMs4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "minimum and max ratings\n",
        "investigate the column\n",
        "'''\n",
        "print (f\"The minimum rating is: {df['rating'].min()}\")\n",
        "print (f\"The maximum rating is: {df['rating'].max()}\")\n",
        "\n",
        "search_character = df.query(\"rating == '|'\")\n",
        "print(tabulate(search_character, headers = 'keys', tablefmt = 'grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MLuA05NQ3pr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Replaced the | character with the rating I found online\n",
        "Source : https://www.amazon.in/Eureka-Forbes-Vacuum-Cleaner-Washable/dp/B08L12N5H1\n",
        "\n",
        "'''\n",
        "#Replace pipe with 1 and cast df to float\n",
        "df['rating']=df['rating'].str.replace(\"|\",\"3.9\").astype(float)\n",
        "df['rating'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn8FCxSCWL4w"
      },
      "outputs": [],
      "source": [
        "#Verify datatypes\n",
        "Ratings = df[['user_id','product_id','product_name','rating','discounted_price',\t'actual_price', 'discount_percentage',\t'rating_count']]\n",
        "Ratings.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1SOwuMicP26"
      },
      "outputs": [],
      "source": [
        "print (\"Total Numerical Data\")\n",
        "print (\"*\"*50)\n",
        "print (f\"Total no of ratings:  {Ratings.shape[0]}\")\n",
        "print (f\"Total No of Users: {len(np.unique(Ratings.user_id))}\")\n",
        "print (f\"Total No of Products: {len(np.unique(Ratings.discounted_price))}\")\n",
        "\n",
        "print(f\"\\nThere area total of {Ratings.shape[0]} user ratings in our dataset from {len(np.unique(Ratings.user_id))} unique users with {len(np.unique(Ratings.discounted_price))} products\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save CLeaned DataSets"
      ],
      "metadata": {
        "id": "fbm1hbahHboU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouYpY_dUwFC2"
      },
      "source": [
        "\n",
        "\n",
        "### **Step 6:** Visualize Data Relationships\n",
        "\n",
        "* Categorical: frequency tables, bar plots, pie charts\n",
        "* Numerical: histograms, box plots, violin plots\n",
        "* Relationships: scatter plots, correlation matrices, statistical tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehyExaSQU6Xf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#most of the ratings are around the mid level from 3.9 -4.4, its generally lesser at the ends\n",
        "with sns.axes_style(\"dark\"):\n",
        "  g = sns.catplot(x = \"rating\", data = Ratings, aspect = 4.0, kind = \"count\")\n",
        "  g.set_ylabels(\"Total number of ratings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYXLCRlHZq04"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.rcParams['patch.force_edgecolor'] = True\n",
        "\n",
        "# Get unique rating_count values\n",
        "unique_ratings = Ratings['rating_count'].unique()\n",
        "\n",
        "# Plot histogram of unique values\n",
        "plt.hist(unique_ratings, bins=50)\n",
        "plt.xlabel(\"Rating Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Unique Rating Counts\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKqseJau0Hwi"
      },
      "source": [
        "# **Recommendations System - Machine learning Approaches**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uyk-cHKhTM_"
      },
      "source": [
        "### **1. Popularity-Based System**\n",
        "**Approach:** Recommend the most popular items based on rating_count.\n",
        "\n",
        "\n",
        "**Evaluation:** Popularity-based systems are often evaluated based on business metrics like sales uplift or user engagement, as they don't personalize recommendations.\n",
        "\n",
        "* Hit rate  $(\\frac{hits}{Users})$\n",
        "\n",
        "* Recall    $(\\frac{Number of Relevant items in Top-k}{Total Number of Relevant Items})$\n",
        "\n",
        "* Precison   $(\\frac{Number of Relevant items in Top-K}{Total Number of items in\n",
        " Top-K Recommended})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvxy8HI4hB2v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame (assuming 'Ratings' is your dataset)\n",
        "df_ratings = Ratings.copy()\n",
        "\n",
        "# Ensure numeric types\n",
        "df_ratings['rating'] = pd.to_numeric(df['rating'])\n",
        "df_ratings['rating_count'] = pd.to_numeric(df['rating_count'])\n",
        "df_ratings['discount_percentage'] = pd.to_numeric(df['discount_percentage'])\n",
        "\n",
        "# Define a weight for discount influence\n",
        "discount_weight = 0.1\n",
        "\n",
        "# Compute Popularity Score\n",
        "df_ratings['popularity_score'] = (df_ratings['rating'] * df_ratings['rating_count']) + (df_ratings['discount_percentage'] * discount_weight)\n",
        "\n",
        "# Sort items based on popularity\n",
        "popular_items = df_ratings.groupby(['product_name','rating'])['popularity_score'].sum().reset_index()\n",
        "popular_items = popular_items.sort_values(by='popularity_score', ascending=False)\n",
        "\n",
        "\n",
        "top_n = 10\n",
        "print(f\"Top {top_n} Popular Products:\")\n",
        "print(tabulate(popular_items.head(top_n),headers = 'keys',tablefmt = 'grid' ))\n",
        "\n",
        "popular_items.head(top_n).to_csv(\"popular_products.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEDYPuDhjB_i"
      },
      "source": [
        "\n",
        "##### Hit_rate, Precison, Recall for popularity based\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjo8NRkcjBoW"
      },
      "outputs": [],
      "source": [
        "def hit_rate(predicted_top_products, actual_top_products, k):\n",
        "  predicted_top_products = set(predicted_top_products[:k])\n",
        "  actual_top_products = set(actual_top_products[:k])\n",
        "  Model_check = len(predicted_top_products & actual_top_products)\n",
        "  Hit_rate = Model_check/k\n",
        "  return Hit_rate\n",
        "\n",
        "\n",
        "\n",
        "def recall(predicted_top_products, actual_top_products, k):\n",
        "    predicted_top_products = set(predicted_top_products[:k])\n",
        "    actual_top_products = set(actual_top_products[:k])  # Adjusted\n",
        "    relevant = len(predicted_top_products & actual_top_products)\n",
        "    recall = relevant / len(actual_top_products) if len(actual_top_products) > 0 else 0\n",
        "    return recall\n",
        "\n",
        "\n",
        "\n",
        "def precision(predicted_top_products, actual_top_products, k):\n",
        "  predicted_top_products = set(predicted_top_products[:k])\n",
        "  actual_top_products = set(actual_top_products[:k])\n",
        "  relevant_items = len(predicted_top_products & actual_top_products)\n",
        "  precision =  relevant_items/len(predicted_top_products)\n",
        "  return precision\n",
        "\n",
        "\n",
        "predicted_top_products = df_ratings.sort_values(by='popularity_score', ascending=False)['product_id'].tolist()\n",
        "actual_top_products = df.sort_values(by='rating_count', ascending=False)['product_id'].tolist()\n",
        "\n",
        "\n",
        "recall = recall(predicted_top_products, actual_top_products, k=20)\n",
        "precision = precision(predicted_top_products, actual_top_products, k=10)\n",
        "hit_rate_10 = hit_rate(predicted_top_products,actual_top_products, k=10)\n",
        "print(f\"Hit Rate (Top 10): {hit_rate_10:.2f}\")\n",
        "print(f\"Precision_Rate: {precision:.2f}\")\n",
        "print(f\"Recall_Rate: {recall:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "with mlflow.start_run(run_name = \"Hit_rate_popularty_RecSys\"):\n",
        "  mlflow.log_metric(\"Hit_rate\",hit_rate_10)\n",
        "  mlflow.log_metric(\"precision\", round(precision,2))\n",
        "  mlflow.log_metric(\"Recall\",round(recall,4))\n",
        "  mlflow.log_param(\"dataset_size\", len(df))\n",
        "  mlflow.set_tag(\"Top n\", \"Metric for popularity model\")\n",
        "  mlflow.log_artifact(\"popular_products.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL9p-NxlhPRS"
      },
      "source": [
        "### **2. Content-Based System**\n",
        "**Approach:** Use item features like category and about_product to recommend similar items.\n",
        "\n",
        "**Evaluation:** Use precision, recall, and F1-score by comparing recommended items to a test set of user interactions.\n",
        "\n",
        "---\n",
        "**Evaluation Metrics**\n",
        "----------\n",
        "**Precision**: The fraction of recommended items that are relevant.  \n",
        "\n",
        "$\n",
        "\\text{Precision} = \\frac{\\text{Number of Relevant Items Recommended}}{\\text{Total Number of Items Recommended}}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "**Recall**: The fraction of relevant items that are recommended.  \n",
        "\n",
        "$\n",
        "\\text{Recall} = \\frac{\\text{Number of Relevant Items Recommended}}{\\text{Total Number of Relevant Items}}\n",
        "$\n",
        "\n",
        "\n",
        "**F1-Score**: The harmonic mean of precision and recall.  \n",
        "\n",
        "$\n",
        "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbS7_xC79w0O"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sepatated the user_id list to contain only one id per row"
      ],
      "metadata": {
        "id": "X_7a3FExQ3iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ratings_cn = df[['user_id','product_id','product_name','rating','discounted_price',\t'actual_price', 'discount_percentage',\t'rating_count']]\n",
        "\n",
        "Ratings_cn.loc[:,'user_id'] = Ratings_cn['user_id'].str.split(',')\n",
        "\n",
        "ratings_df = Ratings_cn.explode('user_id').reset_index(drop=True)\n",
        "\n",
        "ratings_df.head()"
      ],
      "metadata": {
        "id": "NPayn0PKcP2f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Content Based Filtering**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "foMxx_kiqa21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import mlflow\n",
        "from itertools import combinations\n",
        "\n",
        "# Content Based Recommendation System\n",
        "# Assuming df is your input DataFrame\n",
        "ratings_df = df[['user_id', 'product_id', 'product_name', 'rating', 'discounted_price', 'category',\n",
        "                 'about_product', 'actual_price', 'discount_percentage', 'rating_count']].copy()\n",
        "\n",
        "# Handle NaN values and ensure string type\n",
        "ratings_df['category'] = ratings_df['category'].fillna('').astype(str)\n",
        "ratings_df['about_product'] = ratings_df['about_product'].fillna('').astype(str)\n",
        "ratings_df['combined_features'] = ratings_df['category'] + ' ' + ratings_df['about_product']\n",
        "\n",
        "# Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(ratings_df['combined_features'])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "def get_recommendations(product_index, cosine_sim=cosine_sim, n=10):\n",
        "    try:\n",
        "        sim_scores = list(enumerate(cosine_sim[product_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:n+1]  # Exclude the product itself\n",
        "        product_indices = [i[0] for i in sim_scores]\n",
        "        return ratings_df['product_name'].iloc[product_indices].tolist()\n",
        "    except IndexError:\n",
        "        return []\n",
        "\n",
        "# ========= Evaluation Functions for Content-Based Recommendation System =========\n",
        "\n",
        "\n",
        "def precision_at_k(recommended, relevant, k=10):\n",
        "    \"\"\"Calculate precision@k\"\"\"\n",
        "    if not recommended or k <= 0:\n",
        "        return 0\n",
        "    recommended_at_k = recommended[:k]\n",
        "    relevant_set = set(relevant)\n",
        "    hits = len(set(recommended_at_k) & relevant_set)\n",
        "    return hits / k\n",
        "\n",
        "\n",
        "\n",
        "def recall_at_k(recommended, relevant, k=10):\n",
        "    \"\"\"Calculate recall@k\"\"\"\n",
        "    if not recommended or not relevant or k <= 0:\n",
        "        return 0\n",
        "    relevant_set = set(relevant)\n",
        "    hits = len(set(recommended[:k]) & relevant_set)\n",
        "    return hits / len(relevant_set)\n",
        "\n",
        "\n",
        "\n",
        "def f1_at_k(recommended, relevant, k=10):\n",
        "    \"\"\"Calculate F1-score@k\"\"\"\n",
        "    precision = precision_at_k(recommended, relevant, k)\n",
        "    recall = recall_at_k(recommended, relevant, k)\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "\n",
        "def average_precision(recommended, relevant):\n",
        "    relevant_set = set(relevant)  # Unique relevant items\n",
        "    hits = 0\n",
        "    sum_precision = 0\n",
        "    for i, item in enumerate(recommended, 1):\n",
        "        if item in relevant_set and hits < len(relevant_set):  # Cap at unique relevant items\n",
        "            hits += 1\n",
        "            sum_precision += hits / i\n",
        "    return sum_precision / len(relevant_set) if len(relevant_set) > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "def dcg_at_k(recommended, relevant, k):\n",
        "    \"\"\"Calculate Discounted Cumulative Gain\"\"\"\n",
        "    relevant_set = set(relevant)\n",
        "    dcg = 0\n",
        "    for i, item in enumerate(recommended[:k], 1):\n",
        "        if item in relevant_set:\n",
        "            dcg += 1 / np.log2(i + 1)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ndcg_at_k(recommended, relevant, k):\n",
        "    dcg = dcg_at_k(recommended, relevant, k)\n",
        "    sorted_relevant = sorted(relevant, key=lambda x: recommended.index(x) if x in recommended else float('inf'))\n",
        "\n",
        "    ideal_dcg = dcg_at_k(sorted_relevant[:k], relevant, k)\n",
        "\n",
        "    return dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def reciprocal_rank(recommended, relevant):\n",
        "    for i, item in enumerate(recommended, 1):\n",
        "        if item in relevant:\n",
        "            return 1 / i\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "def average_similarity(query_index, recommended_products, cosine_sim):\n",
        "    query_similarities = cosine_sim[query_index]\n",
        "    recommended_indices = ratings_df[ratings_df['product_name'].isin(recommended_products)].index\n",
        "    similarities = [query_similarities[idx] for idx in recommended_indices if idx < len(query_similarities)]\n",
        "    return sum(similarities) / len(similarities) if similarities else 0\n",
        "\n",
        "\n",
        "\n",
        "def diversity(recommended_products, cosine_sim, Ratings_cn):\n",
        "    \"\"\"Calculate intra-list diversity\"\"\"\n",
        "    indices = ratings_df[ratings_df['product_name'].isin(recommended_products)].index.tolist()\n",
        "\n",
        "    if len(indices) < 2:\n",
        "        return 0  # Diversity not defined for <2 items\n",
        "    sim_sum = 0\n",
        "    count = 0\n",
        "    for i, j in combinations(indices, 2):\n",
        "        if i < len(cosine_sim) and j < len(cosine_sim):\n",
        "            sim_sum += cosine_sim[i][j]\n",
        "            count += 1\n",
        "    avg_sim = sim_sum / count if count > 0 else 0\n",
        "    return 1 - avg_sim  # Higher value = more diverse recommendations\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation and MLflow Run\n",
        "sample_product_index = 1\n",
        "sample_product_name = ratings_df['product_name'].iloc[sample_product_index]\n",
        "recommended_products = get_recommendations(sample_product_index, n=10)\n",
        "\n",
        "user_id = ratings_df.iloc[sample_product_index]['user_id']\n",
        "relevant_items = ratings_df[ratings_df['user_id'] == user_id]['product_name'].tolist()\n",
        "\n",
        "# Compute all evaluation metrics\n",
        "k = 10\n",
        "precision = precision_at_k(recommended_products, relevant_items, k)\n",
        "recall = recall_at_k(recommended_products, relevant_items, k)\n",
        "f1_score = f1_at_k(recommended_products, relevant_items, k)\n",
        "map_score = average_precision(recommended_products, relevant_items)\n",
        "ndcg_score = ndcg_at_k(recommended_products, relevant_items, k)\n",
        "mrr_score = reciprocal_rank(recommended_products, relevant_items)\n",
        "avg_sim = average_similarity(sample_product_index, recommended_products, cosine_sim)\n",
        "div_score = diversity(recommended_products, cosine_sim,Ratings_cn)\n",
        "\n",
        "# ============================ MLflow run ==================================\n",
        "with mlflow.start_run(run_name=\"Content_Based_RecSys\"):\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"dataset_size\", len(df))\n",
        "    mlflow.log_param(\"top_n\", k)\n",
        "\n",
        "    # Log artifacts\n",
        "    pd.DataFrame(cosine_sim).to_csv(\"cosine_similarity_matrix.csv\", index=False)\n",
        "    mlflow.log_artifact(\"cosine_similarity_matrix.csv\")\n",
        "\n",
        "    # Log example recommendation\n",
        "    mlflow.log_dict({\"product_name\": sample_product_name, \"recommendations\": recommended_products}, \"recommendations.json\")\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"precision_at_10\", precision)\n",
        "    mlflow.log_metric(\"recall_at_10\", recall)\n",
        "    mlflow.log_metric(\"f1_score_at_10\", f1_score)\n",
        "    mlflow.log_metric(\"map_at_10\", map_score)\n",
        "    mlflow.log_metric(\"ndcg_at_10\", ndcg_score)\n",
        "    mlflow.log_metric(\"mrr\", mrr_score)\n",
        "    mlflow.log_metric(\"average_similarity\", avg_sim)\n",
        "    mlflow.log_metric(\"diversity\", div_score)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Recommendations for {sample_product_name}:\\n\")\n",
        "for index, value in enumerate(recommended_products, start=1):\n",
        "    print(f\"{index}: {value}\")\n",
        "print(f\"\\nPrecision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "print(f\"MAP: {map_score:.4f}\")\n",
        "print(f\"NDCG: {ndcg_score:.4f}\")\n",
        "print(f\"MRR: {mrr_score:.4f}\")\n",
        "print(f\"Average Similarity: {avg_sim:.4f}\")\n",
        "print(f\"Diversity: {div_score:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(\"Relevant items: \\n\")\n",
        "for index,value in enumerate(relevant_items,start = 1):\n",
        "  print(f\"{index} {value}\")"
      ],
      "metadata": {
        "id": "ytAW6rtgg-aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Collabirative Filtering (SVD)**"
      ],
      "metadata": {
        "id": "6lNtPQN15Lbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def precision_recall_f1_at_k(predictions, k=10, threshold=3.5):\n",
        "    user_est_true = defaultdict(list)\n",
        "\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "    precisions = {}\n",
        "    recalls = {}\n",
        "    f1_scores = {}\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_k = user_ratings[:k]\n",
        "\n",
        "        n_relevant = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "        n_recommended = sum((est >= threshold) for (est, _) in top_k)\n",
        "        n_relevant_and_recommended = sum((true_r >= threshold and est >= threshold) for (est, true_r) in top_k)\n",
        "\n",
        "        precisions[uid] = n_relevant_and_recommended / n_recommended if n_recommended else 0\n",
        "        recalls[uid] = n_relevant_and_recommended / n_relevant if n_relevant else 0\n",
        "        f1_scores[uid] = 2 * (precisions[uid] * recalls[uid]) / (precisions[uid] + recalls[uid]) if (precisions[uid] + recalls[uid]) else 0\n",
        "\n",
        "    avg_precision = sum(precisions.values()) / len(precisions) if precisions else 0\n",
        "    avg_recall = sum(recalls.values()) / len(recalls) if recalls else 0\n",
        "    avg_f1 = sum(f1_scores.values()) / len(f1_scores) if f1_scores else 0\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n",
        "\n",
        "\n",
        "def get_top_n_recommendations(user_id, n=10):\n",
        "    all_products = set(data['product_name'].unique())\n",
        "    rated_products = set(data[data['user_id'] == user_id]['product_name'].values)\n",
        "    unseen_products = all_products - rated_products\n",
        "\n",
        "\n",
        "    user_ratings = [\n",
        "        (product, algo.predict(user_id, product).est) for product in unseen_products\n",
        "    ]\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [product for product, _ in user_ratings[:n]]\n",
        "\n",
        "\n",
        "'''\n",
        "Clone dataframe clean it and prepare for recommendations\n",
        "'''\n",
        "data = ratings_df[['user_id', 'product_name', 'rating']].copy()\n",
        "data['rating'] = pd.to_numeric(data['rating'], errors='coerce')\n",
        "data = data.dropna()\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "surprise_data = Dataset.load_from_df(data[['user_id', 'product_name', 'rating']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(surprise_data, test_size=0.2, random_state=303)\n",
        "\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "predictions = algo.test(testset)\n",
        "rmse_svm = accuracy.rmse(predictions)\n",
        "\n",
        "precision, recall, f1 = precision_recall_f1_at_k(predictions, k=4, threshold=3.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Precision@10: {precision:.4f}\")\n",
        "print(f\"Recall@10: {recall:.4f}\")\n",
        "print(f\"F1@10: {f1:.4f}\")\n",
        "\n",
        "top_recommendations = get_top_n_recommendations(user_id, n=7)\n",
        "user_id = 'AEWAZDZZJLQUYVOVGBEUKSLXHQ5A'\n",
        "print(f\"\\nTop 10 recommendations for user {user_id}:\")\n",
        "for index, value in enumerate(top_recommendations):\n",
        "    print(f\"{index + 1}. {value}\")\n",
        "\n",
        "\n",
        "\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"Collaborative Filtering Using SVD RecSys\"):\n",
        "    mlflow.log_metric(\"RMSE\", round(rmse_svm, 4))\n",
        "    mlflow.log_metric(\"Precision\", round(precision, 4))\n",
        "    mlflow.log_metric(\"Recall\", round(recall, 4))\n",
        "    mlflow.log_metric(\"F1\", round(f1, 4))\n",
        "    mlflow.log_param(\"user_id\", user_id)\n",
        "\n",
        "    for index, value in enumerate(top_recommendations):\n",
        "        mlflow.log_param(f\"top_recommendations_{index}\", value)"
      ],
      "metadata": {
        "id": "JYveHT2DYJnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-7smPCizGh"
      },
      "source": [
        "### **4. Hybrid Approach**\n",
        "Approach: Combine content-based and collaborative filtering recommendations.\n",
        "\n",
        "\n",
        "Evaluation: Use a combination of precision, recall, F1-score, and RMSE to evaluate the hybrid system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRbtVpYNlHId"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "def hybrid_recommendations(user_id, product_index, n=10):\n",
        "    # Get content-based recommendations\n",
        "    content_recs = get_recommendations(product_index)\n",
        "\n",
        "    # Get collaborative filtering recommendations\n",
        "    all_product_names = df['product_name'].unique()\n",
        "    user_ratings = [(product, item_based_algo.predict(user_id, product).est) for product in all_product_names]\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    collab_recs = [product for product, rating in user_ratings[:n]]\n",
        "\n",
        "    # Combine and deduplicate recommendations\n",
        "    combined_recs = list(set(content_recs) | set(collab_recs))\n",
        "\n",
        "    return combined_recs[:n]\n",
        "\n",
        "# Start MLflow experiment for logging hybrid recommendations only\n",
        "with mlflow.start_run(run_name = \"hybrid-aproach_RecSys\"):  # Start the run under the experiment\n",
        "    # Log parameters for this run\n",
        "    user_id = 'A1'\n",
        "    product_index = 0\n",
        "    mlflow.log_param(\"user_id\", user_id)\n",
        "    mlflow.log_param(\"product_index\", product_index)\n",
        "\n",
        "    # Get hybrid recommendations\n",
        "    recommendations = hybrid_recommendations(user_id, product_index, n=10)\n",
        "\n",
        "    # Log the hybrid recommendations as a parameter (may be large)\n",
        "    mlflow.log_param(\"hybrid_recommendations\", recommendations)\n",
        "\n",
        "    # Print the product name and the hybrid recommendations\n",
        "    product_name = df.iloc[product_index]['product_name']\n",
        "    print(f\"Hybrid Recommendations for product '{product_name}':\\n\")\n",
        "    for i, rec in enumerate(recommendations, start=1):\n",
        "        print(f\"{i}. {rec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyT8NiNZOjkj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "original\n",
        "4. Hybrid Approach\n",
        "Approach: Combine content-based and collaborative filtering recommendations.\n",
        "\n",
        "\n",
        "Evaluation: Use a combination of precision, recall, F1-score, and RMSE to evaluate the hybrid system.\n",
        "\n",
        "def hybrid_recommendations(user_id, product_index, n=10):\n",
        "    # Get content-based recommendations\n",
        "    content_recs = get_recommendations(product_index)\n",
        "\n",
        "\n",
        "    # Get collaborative filtering recommendations\n",
        "    all_product_names = df['product_name'].unique()\n",
        "    user_ratings = [(product, item_based_algo.predict(user_id, product).est) for product in all_product_names]\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    collab_recs = [product for product, rating in user_ratings[:n]]\n",
        "\n",
        "    # Combine recommendations while preserving order and removing duplicates\n",
        "    combined_recs = content_recs + [item for item in collab_recs if item not in content_recs]\n",
        "\n",
        "\n",
        "    return combined_recs[:n]\n",
        "\n",
        "# Example: Get top 10 hybrid recommendations for a user and a product\n",
        "recommendations = hybrid_recommendations('A1', 44, n=10)\n",
        "print(\"Top 10 Hybrid Recommendations:\")\n",
        "for i, rec in enumerate(recommendations, start=1):\n",
        "    print(f\"{i}. {rec}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Conclusion**\n",
        "\n",
        "* Popularity-Based: Simple and effective for general recommendations.\n",
        "* Content-Based: Personalized based on item features.\n",
        "* Collaborative Filtering: Leverages user behavior for personalized recommendations.\n",
        "* Hybrid: Combines strengths of both content-based and collaborative filtering for improved accuracy and diversity.\n",
        "\n",
        "Each system can be evaluated using appropriate metrics, and the choice of system depends on the specific needs and constraints of your application.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Integrating fair and equitable principles into recommendation systems is essential to prevent harm, promote diversity, and ensure access to opportunities across digital platforms. This requires interdisciplinary collaboration among computer scientists, ethicists, social scientists, and affected stakeholders to design systems that are not only efficient, but just and inclusive.\n"
      ],
      "metadata": {
        "id": "0yiSbxIyRs0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "- Ekstrand, M. (n.d.). *NSF CAREER award on recommenders, humans, and data*. Michael Ekstrand on the Web. [https://md.ekstrandom.net/research/career](https://md.ekstrandom.net/research/career)\n",
        "\n",
        "- Gunawardana, A., & Shani, G. (2009). *A survey of accuracy evaluation metrics of recommendation tasks*. Journal of Machine Learning Research, 10(12).\n",
        "\n",
        "- Abdollahpouri, H., & Burke, R. (2019). *Multi-stakeholder recommendation and its connection to multi-sided fairness*. arXiv preprint [arXiv:1907.13158](https://arxiv.org/abs/1907.13158).\n",
        "\n",
        "- Chokhra, P. (2021, December 28). *Evaluating Recommender Systems | Nerd for tech*. Medium. [https://medium.com/nerd-for-tech/evaluating-recommender-systems-590a7b87afa5](https://medium.com/nerd-for-tech/evaluating-recommender-systems-590a7b87afa5)\n",
        "\n",
        "- Wang, Y., Ma, W., Zhang, M., Liu, Y., & Ma, S. (2023). *A survey on the fairness of recommender systems*. ACM Transactions on Information Systems, 41(3), 1–43.\n",
        "\n",
        "- CS50. (2015, November 11). *Recommender Systems* [Video]. YouTube. [https://www.youtube.com/watch?v=Eeg1DEeWUjA](https://www.youtube.com/watch?v=Eeg1DEeWUjA)\n",
        "\n",
        "- Data Mentor. (2022, July 10). *Learn recommendation systems in 2 hours* [Video]. YouTube. [https://www.youtube.com/watch?v=kPxASj5wJBY](https://www.youtube.com/watch?v=kPxASj5wJBY)\n",
        "\n",
        "- freeCodeCamp.org. (2025, January 27). *LLM course – Build a Semantic Book Recommender (Python, OpenAI, LangChain, Gradio)* [Video]. YouTube. [https://www.youtube.com/watch?v=Q7mS1VHm3Yw](https://www.youtube.com/watch?v=Q7mS1VHm3Yw)\n",
        "\n",
        "- NeuralNine. (2024, June 9). *Movie Recommender System in Python with LLMs* [Video]. YouTube. [https://www.youtube.com/watch?v=epidA1fBFtI](https://www.youtube.com/watch?v=epidA1fBFtI)\n"
      ],
      "metadata": {
        "id": "QGIUaPg6sRX_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}